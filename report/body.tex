\section{Project Description}

We are developing a system for searching scientific articles.

The data for our search engine is collected from online libraries like \url{arxiv.org} 
and from publishers that have information about their articles available online (like \url{springer.com}).

% a large number of pages
Firstly, we crawled \texttt{20k} pages from these sources and saved them for future processing.

Then, we filtered irrelevant pages (those that doesn't contain an article description) and processed the rest of the documents.
For each relevant page, we extracted article's title, abstract, list of authors, date of publishing, etc.
Also, we store a link for a PDF version of an article, if it's available.

After that, we are going to implement a search engine for searching among these articles. \textbf{We will discuss it later}.

The key feature of our service will be creating some kind of graph over authors/articles and their references to each other. \textbf{We will discuss it later}.

\section{System design}
We use \texttt{Python 3}  for development and \texttt{PostgreSQL} database.

Articles, processed articles' abstracts and inverted index are stored in a filesystem.
Articles' metadata and PAF are stored in the database.


\section{Data acquisition}
For data acquisition we implemented a web crawler. \\
The key details of out implementation are:
\begin{enumerate}
    \item
        \textbf{Politeness policy}
        
        We follow the constraints defined in \texttt{robots.txt}.
        We do not visit excluded pages, do not store page if there is a \texttt{noindex} meta tag, and of course, do not spam a website with a lot of queries.
        Even if a delay is not specified in \texttt{robots.txt} we use value defined in the configuration file (default \texttt{delay} is \texttt{50ms}).

    \item
        \textbf{Distributed crawling}
        
        The problem with Python's multithreading is that only one thread can be run at a time because of \texttt{GIL}, so we use multiprocessing instead.
        Each crawler runs in its own process and several crawlers can be run simultaneously.

		\pagebreak
		Crawlers do not exchange URLs. This solution is suitable in our case because:
		\begin{enumerate}
            \item
                There are not so many data sources with scientific articles so we can just list them.
            \item
            	We are not interested in links leading to another domain, because, most likely, this domain will not contain any relevant documents.
        \end{enumerate}
		\

		Also it gives us a few advantages:
		\begin{enumerate}
			\item Simplicity of implementation. It is more difficult to make a mistake.
			\item Easy to run on different computers, because communication is not needed.
			\item Each crawler has its own set of allowed hosts (we define these hosts in the configuration file). It prevents the same page to be downloaded several times.
		\end{enumerate}

\end{enumerate}

We store each HTML page we crawled in a filesystem and put some page's metadata 
(like its URL, hash of page content, date of last page modify) in database. Duplicated pages (with equal hashes) are ignored.

\section{Data processing and storage}

As we have a few data sources and we need a specific information extracted from crawled documents
(like article's title, abstract and list of authors), we implemented a separate data processor for each data source.

Data processing is done as follows:
\begin{enumerate}

\item Firstly, we filter out pages that doesn't contain an article.

\item Then, we extract relevant parts of remained pages and transform it to a plaintext, removing all the HTML tags.

\item After that, article's abstract is processed for future indexing, stemming its words and removing stopwords and punctiation

\end{enumerate}

Raw and processed article's abstract are stored in the filesystem,
other information (like title, list of authors, link to PDF if available) is put into database

We use \texttt{BeautifulSoup} for webpage parsing, and \texttt{NLTK} for abstract processing.

\subsection{Indexing}
\subsubsection{Page attribute file}
For each article, we store its attributes: the title, the path to file with abstract, the path to file with processed abstract, the number of words, the link to pdf and so on.

This information we store in the database.

\subsubsection{Inverted index}
We built an inverted index. For each word, we store a list with documents that contain this word. And for each document, we store the number of occurrences of this word in the document with positions of these occurrences.

The document lists are sorted by the number of occurrences. Also, we use gap values to store positions of these occurrences for better compression.

This index can be build by several processes simultaneously. We split all articles by their \texttt{id} into groups, and each group is processed by a separate process. After that, all indices are merged into one index.

We store this index in a filesystem. We use \texttt{gzip} for compression.